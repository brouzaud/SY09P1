---
title: "Projet 1 SY09"
output: html_notebook
---

##Exercice 1

###Question 1 : Annalyse exploratoire des données
```{r}
recettes <- read.table('donnees/donnees/recettes-pays.data',header = T,sep = ',',row.names = 1)
#summary(recettes)

boxplot(recettes)
```
```{r}
rownames(recettes)
```
On remarque que les différentes individus correspondent à la fréquence d'utilisation de certains aliments dans une région donnée, certains aliments étant bien plus utilisés que d'autres

```{r}
#cor(recettes)
```

###Question 2

```{r}
#setwd('Desktop/SY09P1/')


ACPrecettes <- prcomp(recettes,scale. = T)

plot(ACPrecettes$x[,1:2])
text(ACPrecettes$x[,1:2], rownames(recettes), pos = 1)
plot(ACPrecettes$x[,c(1,3)])
text(ACPrecettes$x[,c(1,3)], rownames(recettes), pos = 1)
plot(ACPrecettes$x[,2:3])
text(ACPrecettes$x[,2:3], rownames(recettes), pos = 1)

```
On distingue deux voire trois groupes de manière plus ou moins distincte
On pourrait y voir les plats Asiatiques, les plats d'Europe du Nord et les plats méditerranéens confondus avec les plats réputés épicés

###Question 3

```{r}
#install.packages("dendextend")
library(dendextend)
hierarchy <- hclust(dist(recettes, method = "manhattan"))
plot(color_labels(hierarchy, k = 5), main = "Dendogrammes des origines")

```
On retrouve les 3 classes décrites avant dans le dendogramme ci-dessus

###Question 4

```{r}
 par(mfrow=c(3,2))

recetteBestKMeans <- list()
for (i in c(2:7)) {
  recetteKMeans <- list()
  for(j in c(1:20)) {
    km <- list(kmeans(recettes, i))
    recetteKMeans <- c(recetteKMeans, km)    
  }

  r <- lapply(recetteKMeans,FUN = function(x) {
    x$tot.withinss
  })
  
  lapply(r,min)
  minIndex <- which.min(unlist(r))
  
  bestKm <- list(recetteKMeans[[minIndex]])
  
  recetteBestKMeans <- c(recetteBestKMeans, bestKm)
}

# print(recetteBestKMeans)

for(i in c(2:7)) {
    plot(ACPrecettes$x[1:26,], col=c("red", "green", "blue", "black",     "orange", "purple", "pink")[recetteBestKMeans[[i - 1]]$cluster])

}
```
On prends à chaque fois la meilleure des clusterisations sur 20 essais et ce pour une clusterisation de 2 à 7 classes. Jusqu'à 5 classes, les resultats sopnt très nets.

### Question 5

```{r}
continent <- c(1:26)
continentClusturedRecettes <-cbind(recettes, continent) 

#ASIA
continentClusturedRecettes[c("Asia", "Chinese", "Thai", "Japanese", "Vietnamese", "Jewish", "MiddleEastern", "Indian"), "continent"] <- 1 

#AFRICA
continentClusturedRecettes[c("African", "Morrocan", "Cajun_Creole"), "continent"] <- 2

#EUROPE
continentClusturedRecettes[c("English_Scottish", "Irish", "French", "Scandinavian", "EasternEuropean_Russian", "German", "Greek", "Italian", "Mediterranean", "Spanish_Portuguese", ""), "continent"] <- 3   

#NA
continentClusturedRecettes[c("America", "Southern_SoolFood", "SouthWestern"), "continent"] <- 4 

#SA
continentClusturedRecettes[c("Mexican", "Central_SouthAmerican"), "continent"] <- 5 

plot(ACPrecettes$x, col=c("red", "green", "blue", "black",     "orange", "purple", "pink" )[continentClusturedRecettes[,"continent"]])
```
On obtient des différences entre les deux groupements
Attention : cette representation repose sur une separation continentale et non culturelles -> expertise métier nécessaire

###Question 6

```{r}

recetteEchant <- read.table('donnees/donnees/recettes-echant.data',header = T,sep = ',')

summary(recetteEchant)
head(recetteEchant)

#summary(recetteEchant[2:51])
#cov(recetteEchant[2:51])
#print(princomp(recetteEchant[2:51]))
```

RecetteEchant est en fait la source qui a été agrégée sur les origines pour la première partie.

En effet, les observations sont composées de 0 et de 1 qui font office de booléen pour savoir si une recette utilise tel ou tel ingrédient.

###Question 7

```{r}
Trecettes <- t(recetteEchant)

originMatrix <- matrix(data = 0, nrow = length(unique(factor(recetteEchant$origin))), ncol = length(recetteEchant[,1]))

row.names(x = originMatrix) <-  unique(factor(recetteEchant$origin))

for (i in c(1:length(recetteEchant[,1]))) {
  originMatrix[Trecettes["origin", i], i] <- 1
}

Trecettes <- rbind(Trecettes[2:51,], originMatrix)

for(i in c(1:length(Trecettes[,1]))) {
  for (j in c(1:length(Trecettes[1,])))
    Trecettes[i,j] <- as.numeric(Trecettes[i,j])
}

#Attention à la métrique utilisée

# TrecettesDist <- dist(Trecettes,method = "binary")

TrecettesDist <- dist(Trecettes[1:50,],method = "binary")
#Ici on ne prends pas les pays en compte
```
Ici on perd toute information relative aux origines.
Quand on ne supprime pas les origines, on trouve des informations aberrantes comme de grosses distances entre la cuisine japonaise et Asiatique


On utilise la distance binnaire puisqu'on a des valeurs psuudo booléenne. Cela permet d'avoir des distances entre 0 et 1 ce qui est logique quant à la matrice de départ.

###Question 8

```{r}

ingredientClusters <- hclust(1 - TrecettesDist)
plot(color_labels(ingredientClusters, k = 4))
```


```{r}
library(cluster)

for(i in c(1:length(Trecettes[,1]))) {
  for (j in c(1:length(Trecettes[1,])))
    Trecettes[i,j] <- as.numeric(Trecettes[i,j])
}

#TODO faire ça sur plusieurs k différends et avec plusieurs itérations de k medoide a chaque fois
Trecettes <- Trecettes[1:50,]


agregation <- aggregate(recetteEchant[,-1], by = list(Origin = recetteEchant$origin), FUN = sum)
recetteAgregation <- (t(as.data.frame(x = agregation, colnames = agregation$Originrigin))[-1,])


# apply(Trecettes, 2, as.numeric)
# sapply(Trecettes, as.numeric)
class(Trecettes) <- "numeric"
storage.mode(Trecettes) <- "numeric"

ACPrecettesEchant <- prcomp(Trecettes[1:50,])

TrecettesDist <- dist(recetteAgregation,method = "manhattan")

PAMrecetteEchant <- cluster::pam(TrecettesDist,k = 3, metric = "manhattan", diss = T)$clustering

for(i in c(1:length(recetteAgregation[,1]))) {
  for (j in c(1:length(recetteAgregation[1,])))
    recetteAgregation[i,j] <- as.numeric(recetteAgregation[i,j])
}


cluster::clusplot(as.data.frame(recetteAgregation),PAMrecetteEchant) 
```

```{r}
head(as.data.frame(recetteAgregation))
```

Cette clusterisation n'est pas très utiles vu le peu d'inertie expliquée par les deux axes
 
```{r}
elbow <- (100 * (ACPrecettesEchant$sdev)^2 / sum(ACPrecettesEchant$sdev^2) )
#TODO as an histogram
plot(elbow)
```
##Exercice 2

```{r}

distXY <- function(X, Y, M=diag(dim(X)[2]))
{
  #Put this shit out of function
  X <- as.matrix(X)
  Y <- as.matrix(Y)

  if (!is.matrix(X))
  {
    X <- matrix(X, nrow=1)
  }
  if (!is.matrix(Y))
  {
    Y <- matrix(Y, nrow=1)
  }
  
  nx <- dim(X)[1]
  ny <- dim(Y)[1]
  h.x <- rowSums((X%*%t(chol(M)))^2)
  h.y <- rowSums((Y%*%t(chol(M)))^2)
  ones.x <- rep(1, nx)
  ones.y <- rep(1, ny)
  
  D2xy <- h.x %*% t(ones.y) - 2 * X %*% M %*% t(Y) + ones.x %*% t(h.y)
}



adaptativeKmeans <- function(X, K, rhoKs = 0, niter = 100, ness = 1, epsilon = 0.00001) {
  J <- 10000000
  result <- list()
  #C'est de la merde change moi ça
  if (rhoKs == 0) {
    rhoKs <- array(1, K)
  }

  for (i in c(1 : ness)) {
    #initialisation
    muks <- X[sample(c(1:dim(X)[1]), size = K, replace = FALSE),]
  
    Vks <- list()
    for (j in c(1:length(rhoKs))) {
        append(Vks, diag(dim(X)[2]))
        Vks[[j]] <- rhoKs[j] ** (-1 / dim(X)[2]) * diag(dim(X)[2])
    }
    itercount <- 0
    muKsPrec <- muks + epsilon + 1
    convCriteria <- 0
    
  
    
    #Computing convergence criteria
    for (j in c(1: K)) {
      # print((muks[j,] - muKsPrec[j,])^2)
      convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
    }
    
    while(itercount < niter && convCriteria > epsilon) {
      
      itercount <- itercount + 1  
      
      #Update Mahalanobis distance
      cluster <- apply(X, MARGIN = 1, FUN = function(x) {
        clusterDists <- list()
        for(j in c(1: K)) {
          
          x <- matrix(x, nrow = dim(X)[2], ncol = 1)
          y <- matrix(as.numeric(muks[j,]), nrow = dim(X)[2], ncol = 1)
          distance <- distXY(t(x), t(y), Vks[[j]])
          
          append(clusterDists, array(0, dim = dim(distance)))
          clusterDists[[j]] <- distance
        }
        which.min(clusterDists)
      })

      #Saving old centers
      muKsPrec <- muks

      #Updating Vks (Manahobis matrix) and centers
      for(j in c(1: K)) {
        Pk <- as.matrix(X[cluster == j,], dimnames = NULL)
        muks[j,] <- apply(Pk, MARGIN = 2, mean)
        mu <- matrix(muks[j,], nrow = dim(X)[2], ncol = 1, dimnames = NULL)
  
        Vk <- apply(Pk, MARGIN = 1, FUN = function(x) {
          Xmat <- matrix(x, nrow = dim(X)[2], ncol = 1, dimnames = NULL)

          tmp_diff <- (as.numeric(Xmat) - as.numeric(mu))
          tmp_diff <- matrix(tmp_diff, nrow = dim(X)[2], ncol = 1)
          tmp_diff <- tmp_diff %*% t(tmp_diff)
        })
        #############################################
        #BUG ICI DES FOIS sur le rowMeans
        rMeanVk <- matrix(rowMeans(x = Vk), ncol = dim(X)[2], nrow = dim(X)[2])
        Vk <- matrix(rMeanVk, ncol = dim(X)[2], nrow = dim(X)[2])
        #############################################
        
        Vks[[j]] <- ( (rhoKs[j] * det(Vk))^(-1/dim(X)[2]) ) * Vk
      }

      #Calculate Convergence criteria
      convCriteria <- 0
      
      for (j in c(1: K)) {
        convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
      }

      #Calculate totalDist
      dtot <- array(0, K)
      for(j in c(1: K)) {
        Pk <- as.matrix(X[cluster == j,])
        dtot[j] <- dtot[j] + sum(apply(Pk, MARGIN = 1, FUN = function(x) {
          x <- matrix(x, nrow = dim(X)[2], ncol = 1)
          y <- matrix(as.numeric(muks[j,]), nrow = dim(X)[2], ncol = 1)
          distXY(t(x), t(y), Vks[[j]])
        }))
      }
    }
    Jprec <- J
    J <- sum(dtot)
    if(J < Jprec) {
      result <- list("J" = J,"i" = i,"cluster" = cluster,"centers" = muks,"CovMatrices" = Vks)
    }
  }
  return(result)
}

```

```{r}
#library(mclust)
par(mfrow=c(1,2))

X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]

plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])



```

```{r}
par(mfrow=c(1,2))

X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth2.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]

plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])

```

```{r}
par(mfrow=c(1,2))

X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth3.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]

plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])

```


```{r}
#TODO verifier que Nb essai marchez bien (c'est pas encore le cas)
```

  
```{r}
par(mfrow=c(4,2))

data(iris)
X <- iris[,1:4]
z <- iris[,5]
  
ACPIris <- princomp(X)

for (i in c(2:5)) {
  km <- kmeans(X, i)
  plot(ACPIris$scores[,1:2], col = c("red", "blue", "green", "black", "orange")[km$cluster])
  title(toString(km$tot.withinss))
  
  akm <- adaptativeKmeans(X, i)
  plot(ACPIris$scores[,1:2], col = c("red", "blue", "green", "black", "orange")[akm$cluster])
  title(akm$J)
}
```
Bon ca marche pas tout le temps à cause des def positive mais a force d'essayer ca passe et l'inertie est pas ouf
Plus étonnant encore, quand on fout des print() ca marche plus souvent ... Il se peut que R parrallelise les calculs et que certaines matrices incolmplètes soient utilisées


Faut aussi voir si le ness fait varier (la proba de pas avoir d'erreur va baisser également)

A faire pour le kmean aussi

```{r}

Spam <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/spam.csv", header=T, row.names=1)
X <- Spam[,-58]
z <- Spam[,58]


X <- scale(X)

X <- X[sample(c(1:dim(X)[1]), size = 300, replace = FALSE),]

ACPSpam <- princomp(X)

plot(ACPSpam$scores[,1:2])

```

```{r}
spamPlot <- ACPSpam$scores[,1:2]

spamPlot <- spamPlot - colMeans(spamPlot)

dist <- spamPlot[,1] * spamPlot[,1] + spamPlot[,2] * spamPlot[,2]

dist <- matrix(dist, ncol = 1)

spamPlot <- as.matrix(spamPlot)

spamPlot <- cbind(spamPlot, dist)

indexesToRemove <- order(spamPlot[,3],decreasing = TRUE)[1:30]

indexesToRemove <- - indexesToRemove

spamPlot <- spamPlot[indexesToRemove,]

plot(spamPlot, col = c("red", "blue")[adaptativeKmeans(spamPlot[,1:2], 2)$cluster])
```

Il faut tej les valeurs aberrrantes (TP de term en chimie)

(essayer 3DPLOT)

## Justification

```{r}

```



