---
title: "Projet 1 SY09"
output: html_notebook
---

##Exercice 1

```{r}
#setwd('Desktop/SY09P1/')
recettes <- read.table('donnees/donnees/recettes-pays.data',header = T,sep = ',',row.names = 1)

recettesX2 <- rbind(recettes[1:50], recettes[1:50])

ACPrecettes <- princomp(recettesX2, scores = T)

plot(ACPrecettes$scores[,1:2])

```

```{r}
#ACPrecettes <- princomp(t(recettes), scores = T)

#plot(ACPrecettes$scores[,1:2])

```

```{r}
hierarchy <- hclust(dist(recettes, method = "manhattan"))
plot(hierarchy)
```

```{r}
 par(mfrow=c(3,2))

recetteBestKMeans <- list()
for (i in c(2:7)) {
  recetteKMeans <- list()
  for(j in c(1:20)) {
    km <- list(kmeans(recettes, i))
    recetteKMeans <- c(recetteKMeans, km)    
  }

  r <- lapply(recetteKMeans,FUN = function(x) {
    x$tot.withinss
  })
  
  lapply(r,min)
  minIndex <- which.min(unlist(r))
  
  bestKm <- list(recetteKMeans[[minIndex]])
  
  recetteBestKMeans <- c(recetteBestKMeans, bestKm)
}

# print(recetteBestKMeans)

for(i in c(2:7)) {
    plot(ACPrecettes$scores[1:26,], col=c("red", "green", "blue", "black",     "orange", "purple", "pink")[recetteBestKMeans[[i - 1]]$cluster])

}
```

```{r}
continent <- c(1:26)
continentClusturedRecettes <-cbind(recettes, continent) 

#ASIA
continentClusturedRecettes[c("Asia", "Chinese", "Thai", "Japanese", "Vietnamese", "Jewish", "MiddleEastern", "Indian"), "continent"] <- 1 

#AFRICA
continentClusturedRecettes[c("African", "Morrocan", "Cajun_Creole"), "continent"] <- 2

#EUROPE
continentClusturedRecettes[c("English_Scottish", "Irish", "French", "Scandinavian", "EasternEuropean_Russian", "German", "Greek", "Italian", "Mediterranean", "Spanish_Portuguese", ""), "continent"] <- 3   

#NA
continentClusturedRecettes[c("America", "Southern_SoolFood", "SouthWestern"), "continent"] <- 4 

#SA
continentClusturedRecettes[c("Mexican", "Central_SouthAmerican"), "continent"] <- 5 

plot(ACPrecettes$scores, col=c("red", "green", "blue", "black",     "orange", "purple", "pink" )[continentClusturedRecettes[,"continent"]])
```

Attention : cette representation repose sur une separation continentale et non culturelles -> expertise métier nécessaire

```{r}

recetteEchant <- read.table('donnees/donnees/recettes-echant.data',header = T,sep = ',')

#summary(recetteEchant[2:51])
# cov(recetteEchant[2:51])
#print(princomp(recetteEchant[2:51]))
```

Exemples d'annalyse exploratoire

```{r}
Trecettes <- t(recetteEchant)

originMatrix <- matrix(data = 0, nrow = length(unique(factor(recetteEchant$origin))), ncol = length(recetteEchant[,1]))

row.names(x = originMatrix) <-  unique(factor(recetteEchant$origin))

for (i in c(1:length(recetteEchant[,1]))) {
  originMatrix[Trecettes["origin", i], i] <- 1
}

Trecettes <- rbind(Trecettes[2:51,], originMatrix)

for(i in c(1:length(Trecettes[,1]))) {
  for (j in c(1:length(Trecettes[1,])))
    Trecettes[i,j] <- as.numeric(Trecettes[i,j])
}

#Attention à la métrique utilisée

# TrecettesDist <- dist(Trecettes,method = "binary")

TrecettesDist <- dist(Trecettes[1:50,],method = "binary")
#Ici on ne prends pas les pays en compte

```


```{r}

ingredientClusters <- hclust(1 - TrecettesDist)
plot(ingredientClusters)
```
Quand on ne supprime pas les origines, on a des trucs aberrants du genre grosse distance entre la cuisine japonaise et Asiatique

```{r}
library(cluster)

for(i in c(1:length(Trecettes[,1]))) {
  for (j in c(1:length(Trecettes[1,])))
    Trecettes[i,j] <- as.numeric(Trecettes[i,j])
}

#TODO faire ça sur plusieurs k différends et avec plusieurs itérations de k medoide a chaque fois
Trecettes <- Trecettes[1:50,]

# apply(Trecettes, 2, as.numeric)
# sapply(Trecettes, as.numeric)
class(Trecettes) <- "numeric"
storage.mode(Trecettes) <- "numeric"

ACPrecettesEchant <- prcomp(Trecettes)
plot(ACPrecettesEchant$x[,c(1,2)], col=c("red", "green", "blue", "black",     "orange", "purple", "pink")[cluster::pam(TrecettesDist,3)$clustering])
```

 
```{r}
elbow <- (100 * (ACPrecettesEchant$sdev)^2 / sum(ACPrecettesEchant$sdev^2) )
#TODO as an histogram
plot(elbow)
```
##Exercice 2

```{r}

distXY <- function(X, Y, M=diag(dim(X)[2]))
{
  #Put this shit out of function
  X <- as.matrix(X)
  Y <- as.matrix(Y)
  
  print(M)
  
  if (!is.matrix(X))
  {
    X <- matrix(X, nrow=1)
  }
  if (!is.matrix(Y))
  {
    Y <- matrix(Y, nrow=1)
  }
  
  nx <- dim(X)[1]
  ny <- dim(Y)[1]
  h.x <- rowSums((X%*%t(chol(M)))^2)
  h.y <- rowSums((Y%*%t(chol(M)))^2)
  ones.x <- rep(1, nx)
  ones.y <- rep(1, ny)

  D2xy <- h.x %*% t(ones.y) - 2 * X %*% M %*% t(Y) + ones.x %*% t(h.y)
}





adaptativeKmeans <- function(X, K, rhoKs = 0, niter = 100, ness = 1, epsilon = 0.00001) {
  J <- 10000000
  result <- list()
  #C'est de la merde change moi ça
  if (rhoKs == 0) {
    rhoKs <- array(1, K)
  }

  for (i in c(1 : ness)) {
    #initialisation
    muks <- X[sample(c(1:dim(X)[1]), size = K, replace = FALSE),]
  
    Vks <- list()
    for (j in c(1:length(rhoKs))) {
        append(Vks, diag(dim(X)[2]))
        Vks[[j]] <- rhoKs[j] ** (-1 / dim(X)[2]) * diag(dim(X)[2])
    }
    itercount <- 0
    muKsPrec <- muks + epsilon + 1
    convCriteria <- 0
    
  
    
    #Computing convergence criteria
    for (j in c(1: K)) {
      # print((muks[j,] - muKsPrec[j,])^2)
      convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
    }
    
    while(itercount < niter && convCriteria > epsilon) {
      
      itercount <- itercount + 1  
      
      #Update Mahalanobis distance
      cluster <- apply(X, MARGIN = 1, FUN = function(x) {
        clusterDists <- list()
        for(j in c(1: K)) {
          
          x <- matrix(x, nrow = dim(X)[2], ncol = 1)
          y <- matrix(as.numeric(muks[j,]), nrow = dim(X)[2], ncol = 1)
          distance <- distXY(t(x), t(y), Vks[[j]])
          
          append(clusterDists, array(0, dim = dim(distance)))
          clusterDists[[j]] <- distance
        }
        which.min(clusterDists)
      })

      #Saving old centers
      muKsPrec <- muks

      #Updating Vks (Manahobis matrix) and centers
      for(j in c(1: K)) {
        Pk <- as.matrix(X[cluster == j,], dimnames = NULL)
        muks[j,] <- apply(Pk, MARGIN = 2, mean)
        mu <- matrix(muks[j,], nrow = dim(X)[2], ncol = 1, dimnames = NULL)
  
        Vk <- apply(Pk, MARGIN = 1, FUN = function(x) {
          Xmat <- matrix(x, nrow = dim(X)[2], ncol = 1, dimnames = NULL)

          tmp_diff <- (as.numeric(Xmat) - as.numeric(mu))
          tmp_diff <- matrix(tmp_diff, nrow = dim(X)[2], ncol = 1)
          tmp_diff <- tmp_diff %*% t(tmp_diff)
        })
        # print(Vk)
        Vk <- matrix(rowMeans(Vk), ncol = dim(X)[2], nrow = dim(X)[2])
        # print(( (rhoKs[j] * det(Vk))^(-1/dim(X)[2]) ) * Vk)
        Vks[[j]] <- ( (rhoKs[j] * det(Vk))^(-1/dim(X)[2]) ) * Vk
      }

      #Calculate Convergence criteria
      convCriteria <- 0
      
      for (j in c(1: K)) {
        convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
      }

      #Calculate totalDist
      dtot <- array(0, K)
      for(j in c(1: K)) {
        Pk <- as.matrix(X[cluster == j,])
        dtot[j] <- dtot[j] + sum(apply(Pk, MARGIN = 1, FUN = function(x) {
          x <- matrix(x, nrow = dim(X)[2], ncol = 1)
          y <- matrix(as.numeric(muks[j,]), nrow = dim(X)[2], ncol = 1)
          distXY(t(x), t(y), Vks[[j]])
        }))
      }
    }
    Jprec <- J
    J <- sum(dtot)
    if(J < Jprec) {
      result <- list("J" = J,"i" = i,"cluster" = cluster,"centers" = muks,"CovMatrices" = Vks)
    }
  }
  return(result)
}

```

```{r}
#library(mclust)
par(mfrow=c(1,2))

X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]

plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])



```

```{r}
par(mfrow=c(1,2))

X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth2.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]

plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])

```

```{r}
par(mfrow=c(1,2))

X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth3.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]

plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])

```


```{r}
#TODO verifier que Nb essai marchez bien (c'est pas encore le cas)
```

  
```{r}
par(mfrow=c(4,2))

data(iris)
X <- iris[,1:4]
z <- iris[,5]
  
ACPIris <- princomp(X)

for (i in c(2:5)) {
  km <- kmeans(X, i)
  plot(ACPIris$scores[,1:2], col = c("red", "blue", "green", "black", "orange")[km$cluster])
  title(toString(km$tot.withinss))
  
  akm <- adaptativeKmeans(X, i)
  plot(ACPIris$scores[,1:2], col = c("red", "blue", "green", "black", "orange")[akm$cluster])
  title(akm$J)
}
```
Bon ca marche pas tout le temps à cause des def positive mais a force d'essayer ca passe et l'inertie est pas ouf

```{r}

```





