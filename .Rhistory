plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
distXY <- function(X, Y, M=diag(dim(X)[2]))
{
#Put this shit out of function
Y <- as.matrix(Y)
if (!is.matrix(X))
{
X <- matrix(X, nrow=1)
}
if (!is.matrix(Y))
{
Y <- matrix(Y, nrow=1)
}
nx <- dim(X)[1]
ny <- dim(Y)[1]
h.x <- rowSums((X%*%t(chol(M)))^2)
h.y <- rowSums((Y%*%t(chol(M)))^2)
ones.x <- rep(1, nx)
ones.y <- rep(1, ny)
D2xy <- h.x %*% t(ones.y) - 2 * X %*% M %*% t(Y) + ones.x %*% t(h.y)
}
adaptativeKmeans <- function(X, K, rhoKs = 0, niter = 100, ness = 1, epsilon = 0.00001) {
J <- 10000000
result <- list()
#C'est de la merde change moi ça
if (rhoKs == 0) {
rhoKs <- array(1, K)
}
for (i in c(1 : ness)) {
#initialisation
muks <- X[sample(c(1:dim(X)[1]), size = 2, replace = FALSE),]
Vks <- list()
for (j in c(1:length(rhoKs))) {
append(Vks, diag(dim(X)[2]))
Vks[[j]] <- rhoKs[j] ** (-1 / dim(X)[2]) * diag(dim(X)[2])
}
itercount <- 0
muKsPrec <- muks + epsilon + 1
convCriteria <- 0
#Computing convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
while(itercount < niter && convCriteria > epsilon) {
#Update Mahalanobis distance
cluster <- apply(X, MARGIN = 1, FUN = function(x) {
clusterDists <- list()
for(j in c(1: length(muks))) {
distance <- distXY(X = x, Y = muks[j,], M = Vks[[j]])
append(clusterDists, array(0, dim = dim(distance)))
clusterDists[[j]] <-distance
}
which.min(clusterDists)
})
#Saving old centers
muKsPrec <- muks
#Updating Vks (Manahobis matrix) and centers
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
muks[j,] <- apply(Pk, MARGIN = 2, mean)
xxx <- as.matrix(apply(Pk, MARGIN = 1, FUN = function(x) {x - muks[j,]}))
print(xxx)
xxx <- apply(xxx, MARGIN = 1, FUN = function(x){
return(t(as.matrix(x[[1]])) %*% as.matrix(x[[1]]))
})
Vk <- matrix(0, dim(X)[2], dim(X)[2])
for(mat in xxx) {
Vk <- Vk + (1/length(Pk)) * mat
}
# Vk <- apply(Pk, MARGIN = 1, FUN = function(x) {
#   return( t(x - as.matrix(muks[j,])) %*% (x - as.matrix(muks[j,])) )
# })
Vks[[j]] <- ( (rhoKs[j] * determinant(Vk)) ^ (-1/dim(X)[2]) ) * Vk
}
#Calculate totalDist
dtot <- array(0, length(muks))
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
dtot[j] <- dtot[j] + sum(apply(Pk, FUN = function(x) {
distXY(x, muks[j,], Vks[[j]])
}))
}
#Calculate Convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
}
Jprec <- J
J <- sum(dtot)
if(J < Jprec) {
result <- list(J, i, cluster, muks, Vks)
}
}
return(result)
}
#library(mclust)
par(mfrow=c(3,2))
X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
# plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
#library(mclust)
par(mfrow=c(3,2))
X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
# plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
#library(mclust)
par(mfrow=c(3,2))
X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
# plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
#library(mclust)
par(mfrow=c(3,2))
X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
# plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
distXY <- function(X, Y, M=diag(dim(X)[2]))
{
#Put this shit out of function
Y <- as.matrix(Y)
if (!is.matrix(X))
{
X <- matrix(X, nrow=1)
}
if (!is.matrix(Y))
{
Y <- matrix(Y, nrow=1)
}
nx <- dim(X)[1]
ny <- dim(Y)[1]
h.x <- rowSums((X%*%t(chol(M)))^2)
h.y <- rowSums((Y%*%t(chol(M)))^2)
ones.x <- rep(1, nx)
ones.y <- rep(1, ny)
D2xy <- h.x %*% t(ones.y) - 2 * X %*% M %*% t(Y) + ones.x %*% t(h.y)
}
adaptativeKmeans <- function(X, K, rhoKs = 0, niter = 100, ness = 1, epsilon = 0.00001) {
J <- 10000000
result <- list()
#C'est de la merde change moi ça
if (rhoKs == 0) {
rhoKs <- array(1, K)
}
for (i in c(1 : ness)) {
#initialisation
muks <- X[sample(c(1:dim(X)[1]), size = 2, replace = FALSE),]
Vks <- list()
for (j in c(1:length(rhoKs))) {
append(Vks, diag(dim(X)[2]))
Vks[[j]] <- rhoKs[j] ** (-1 / dim(X)[2]) * diag(dim(X)[2])
}
itercount <- 0
muKsPrec <- muks + epsilon + 1
convCriteria <- 0
#Computing convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
while(itercount < niter && convCriteria > epsilon) {
#Update Mahalanobis distance
cluster <- apply(X, MARGIN = 1, FUN = function(x) {
clusterDists <- list()
for(j in c(1: length(muks))) {
distance <- distXY(X = x, Y = muks[j,], M = Vks[[j]])
append(clusterDists, array(0, dim = dim(distance)))
clusterDists[[j]] <-distance
}
which.min(clusterDists)
})
#Saving old centers
muKsPrec <- muks
#Updating Vks (Manahobis matrix) and centers
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
muks[j,] <- apply(Pk, MARGIN = 2, mean)
print(muks[j,])
###################################################
xxx <- as.matrix(apply(Pk, MARGIN = 1, FUN = function(x) {x - muks[j,]}))
print(xxx)
#######################################################
# Vk <- apply(Pk, MARGIN = 1, FUN = function(x) {
#   return( t(x - as.matrix(muks[j,])) %*% (x - as.matrix(muks[j,])) )
# })
#######################################################
Vks[[j]] <- ( (rhoKs[j] * determinant(Vk)) ^ (-1/dim(X)[2]) ) * Vk
}
#Calculate totalDist
dtot <- array(0, length(muks))
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
dtot[j] <- dtot[j] + sum(apply(Pk, FUN = function(x) {
distXY(x, muks[j,], Vks[[j]])
}))
}
#Calculate Convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
}
Jprec <- J
J <- sum(dtot)
if(J < Jprec) {
result <- list(J, i, cluster, muks, Vks)
}
}
return(result)
}
#library(mclust)
par(mfrow=c(3,2))
X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
# plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
distXY <- function(X, Y, M=diag(dim(X)[2]))
{
#Put this shit out of function
Y <- as.matrix(Y)
if (!is.matrix(X))
{
X <- matrix(X, nrow=1)
}
if (!is.matrix(Y))
{
Y <- matrix(Y, nrow=1)
}
nx <- dim(X)[1]
ny <- dim(Y)[1]
h.x <- rowSums((X%*%t(chol(M)))^2)
h.y <- rowSums((Y%*%t(chol(M)))^2)
ones.x <- rep(1, nx)
ones.y <- rep(1, ny)
D2xy <- h.x %*% t(ones.y) - 2 * X %*% M %*% t(Y) + ones.x %*% t(h.y)
}
adaptativeKmeans <- function(X, K, rhoKs = 0, niter = 100, ness = 1, epsilon = 0.00001) {
J <- 10000000
result <- list()
#C'est de la merde change moi ça
if (rhoKs == 0) {
rhoKs <- array(1, K)
}
for (i in c(1 : ness)) {
#initialisation
muks <- X[sample(c(1:dim(X)[1]), size = 2, replace = FALSE),]
Vks <- list()
for (j in c(1:length(rhoKs))) {
append(Vks, diag(dim(X)[2]))
Vks[[j]] <- rhoKs[j] ** (-1 / dim(X)[2]) * diag(dim(X)[2])
}
itercount <- 0
muKsPrec <- muks + epsilon + 1
convCriteria <- 0
#Computing convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
while(itercount < niter && convCriteria > epsilon) {
#Update Mahalanobis distance
cluster <- apply(X, MARGIN = 1, FUN = function(x) {
clusterDists <- list()
for(j in c(1: length(muks))) {
distance <- distXY(X = x, Y = muks[j,], M = Vks[[j]])
append(clusterDists, array(0, dim = dim(distance)))
clusterDists[[j]] <-distance
}
which.min(clusterDists)
})
#Saving old centers
muKsPrec <- muks
#Updating Vks (Manahobis matrix) and centers
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
muks[j,] <- apply(Pk, MARGIN = 2, mean)
mu <- matrix(muks[j,], dim = c(2,1))
print(mu)
###################################################
xxx <- as.matrix(apply(Pk, MARGIN = 1, FUN = function(x) {x - muks[j,]}))
print(xxx)
#######################################################
# Vk <- apply(Pk, MARGIN = 1, FUN = function(x) {
#   return( t(x - as.matrix(muks[j,])) %*% (x - as.matrix(muks[j,])) )
# })
#######################################################
Vks[[j]] <- ( (rhoKs[j] * determinant(Vk)) ^ (-1/dim(X)[2]) ) * Vk
}
#Calculate totalDist
dtot <- array(0, length(muks))
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
dtot[j] <- dtot[j] + sum(apply(Pk, FUN = function(x) {
distXY(x, muks[j,], Vks[[j]])
}))
}
#Calculate Convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
}
Jprec <- J
J <- sum(dtot)
if(J < Jprec) {
result <- list(J, i, cluster, muks, Vks)
}
}
return(result)
}
#library(mclust)
par(mfrow=c(3,2))
X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
# plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
distXY <- function(X, Y, M=diag(dim(X)[2]))
{
#Put this shit out of function
Y <- as.matrix(Y)
if (!is.matrix(X))
{
X <- matrix(X, nrow=1)
}
if (!is.matrix(Y))
{
Y <- matrix(Y, nrow=1)
}
nx <- dim(X)[1]
ny <- dim(Y)[1]
h.x <- rowSums((X%*%t(chol(M)))^2)
h.y <- rowSums((Y%*%t(chol(M)))^2)
ones.x <- rep(1, nx)
ones.y <- rep(1, ny)
D2xy <- h.x %*% t(ones.y) - 2 * X %*% M %*% t(Y) + ones.x %*% t(h.y)
}
adaptativeKmeans <- function(X, K, rhoKs = 0, niter = 100, ness = 1, epsilon = 0.00001) {
J <- 10000000
result <- list()
#C'est de la merde change moi ça
if (rhoKs == 0) {
rhoKs <- array(1, K)
}
for (i in c(1 : ness)) {
#initialisation
muks <- X[sample(c(1:dim(X)[1]), size = 2, replace = FALSE),]
Vks <- list()
for (j in c(1:length(rhoKs))) {
append(Vks, diag(dim(X)[2]))
Vks[[j]] <- rhoKs[j] ** (-1 / dim(X)[2]) * diag(dim(X)[2])
}
itercount <- 0
muKsPrec <- muks + epsilon + 1
convCriteria <- 0
#Computing convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
while(itercount < niter && convCriteria > epsilon) {
#Update Mahalanobis distance
cluster <- apply(X, MARGIN = 1, FUN = function(x) {
clusterDists <- list()
for(j in c(1: length(muks))) {
distance <- distXY(X = x, Y = muks[j,], M = Vks[[j]])
append(clusterDists, array(0, dim = dim(distance)))
clusterDists[[j]] <-distance
}
which.min(clusterDists)
})
#Saving old centers
muKsPrec <- muks
#Updating Vks (Manahobis matrix) and centers
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
muks[j,] <- apply(Pk, MARGIN = 2, mean)
mu <- matrix(muks[j,], nrow = 2, ncol = 1, dimnames = NULL)
print(mu)
#######################################################
# Vk <- apply(Pk, MARGIN = 1, FUN = function(x) {
#   return( t(x - as.matrix(muks[j,])) %*% (x - as.matrix(muks[j,])) )
# })
#######################################################
Vks[[j]] <- ( (rhoKs[j] * determinant(Vk)) ^ (-1/dim(X)[2]) ) * Vk
}
#Calculate totalDist
dtot <- array(0, length(muks))
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
dtot[j] <- dtot[j] + sum(apply(Pk, FUN = function(x) {
distXY(x, muks[j,], Vks[[j]])
}))
}
#Calculate Convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
}
Jprec <- J
J <- sum(dtot)
if(J < Jprec) {
result <- list(J, i, cluster, muks, Vks)
}
}
return(result)
}
#library(mclust)
par(mfrow=c(3,2))
X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
# plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
distXY <- function(X, Y, M=diag(dim(X)[2]))
{
#Put this shit out of function
Y <- as.matrix(Y)
if (!is.matrix(X))
{
X <- matrix(X, nrow=1)
}
if (!is.matrix(Y))
{
Y <- matrix(Y, nrow=1)
}
nx <- dim(X)[1]
ny <- dim(Y)[1]
h.x <- rowSums((X%*%t(chol(M)))^2)
h.y <- rowSums((Y%*%t(chol(M)))^2)
ones.x <- rep(1, nx)
ones.y <- rep(1, ny)
D2xy <- h.x %*% t(ones.y) - 2 * X %*% M %*% t(Y) + ones.x %*% t(h.y)
}
adaptativeKmeans <- function(X, K, rhoKs = 0, niter = 100, ness = 1, epsilon = 0.00001) {
J <- 10000000
result <- list()
#C'est de la merde change moi ça
if (rhoKs == 0) {
rhoKs <- array(1, K)
}
for (i in c(1 : ness)) {
#initialisation
muks <- X[sample(c(1:dim(X)[1]), size = 2, replace = FALSE),]
Vks <- list()
for (j in c(1:length(rhoKs))) {
append(Vks, diag(dim(X)[2]))
Vks[[j]] <- rhoKs[j] ** (-1 / dim(X)[2]) * diag(dim(X)[2])
}
itercount <- 0
muKsPrec <- muks + epsilon + 1
convCriteria <- 0
#Computing convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
while(itercount < niter && convCriteria > epsilon) {
#Update Mahalanobis distance
cluster <- apply(X, MARGIN = 1, FUN = function(x) {
clusterDists <- list()
for(j in c(1: length(muks))) {
distance <- distXY(X = x, Y = muks[j,], M = Vks[[j]])
append(clusterDists, array(0, dim = dim(distance)))
clusterDists[[j]] <-distance
}
which.min(clusterDists)
})
#Saving old centers
muKsPrec <- muks
#Updating Vks (Manahobis matrix) and centers
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,], dimnames = NULL)
muks[j,] <- apply(Pk, MARGIN = 2, mean)
mu <- matrix(muks[j,], nrow = 2, ncol = 1, dimnames = NULL)
print(dim(Pk))
#######################################################
# Vk <- apply(Pk, MARGIN = 1, FUN = function(x) {
#   return( t(x - as.matrix(muks[j,])) %*% (x - as.matrix(muks[j,])) )
# })
#######################################################
Vks[[j]] <- ( (rhoKs[j] * determinant(Vk)) ^ (-1/dim(X)[2]) ) * Vk
}
#Calculate totalDist
dtot <- array(0, length(muks))
for(j in c(1: length(muks))) {
Pk <- as.matrix(X[cluster == j,])
dtot[j] <- dtot[j] + sum(apply(Pk, FUN = function(x) {
distXY(x, muks[j,], Vks[[j]])
}))
}
#Calculate Convergence criteria
for (j in c(1: length(muks))) {
convCriteria <- convCriteria + sum((muks[j,] - muKsPrec[j,])^2)
}
}
Jprec <- J
J <- sum(dtot)
if(J < Jprec) {
result <- list(J, i, cluster, muks, Vks)
}
}
return(result)
}
#library(mclust)
par(mfrow=c(3,2))
X <- read.csv("/home/arnaud/Desktop/SY09P1/donnees/donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
# plot(X, col = c("red", "blue")[kmeans(X, 2)$cluster])
plot(X, col = c("red", "blue")[adaptativeKmeans(X, 2)$cluster])
